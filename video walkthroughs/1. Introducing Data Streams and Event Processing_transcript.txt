Hi and welcome to this guided project "Performing Real Time
Analytics
with Stream Analytics".
My name is Dean, and I will be your instructor
for this project, which is for people who are interested
in using Azure Stream Analytics.
I will teach you how to use Azure Stream Analytics
and you don't need to have any experience with it, but you
should be familiar with the basics on data stream processing.
I am excited to teach you how to effectively use Azure
Stream Analytics. I have worked with for several companies
within the IT industry and I worked as Azure Data Architect.
Let's take a look at what you'll accomplish by the end
of this project.
By the end of this project, you will learn the following: 
data streams and event processing, data ingestion with event hubs
and processing data with Stream Analytics Jobs.
So let's start with this task.
As more data is generated from a variety of connected devices,
sensors and applications, transforming this data into actionable
insights in near real time is now an operational imperative.
Azure Stream Analytics integrates your streaming data with a
streaming analytics engine to gain insights with streaming
data in real time.
In the context of analytics, data streams are event data
generated by sensors or other sources that can be analyzed
by another technology.
Analyzing a data stream is typically done to measure
the state change of a component or to capture information
on an area of interest.
The process of consuming data streams, analyzing them
and deriving actionable insights out of them is called them
Event Processing. It requires an event producer event processor
and event consumer.
Azure Stream Analytics provides the event processing aspect to
streaming that is fully managed and highly reliable.
We will now start with the demo. You can follow along with me,
and the first step is to open Google Chrome, which is on your
desktop.
Just double click it and log in into the Azure portal
with your account. I have logged in now,
this is my user account. Next, type in your password.
Once you enter the password, click sign in.
In this pop-up click No and you are in the Azure portal.
In this demo we will create Azure Stream Analytics Job
that this data from one storage account, source storage
account, and write it into
a destination storage account.
And that's it.
So in the first two steps, we will create
first source storage account and then we will create
destination storage account.
So first click on Create Resource.
Type in Storage and click on Storage account.
Click create, under a Resource group
choose your resource group and for the storage account name,
type in something that is globally unique.
So I will enter SA storage account,
SRC as source and dsavovic
 as my name.
Location for myself is West Europe because this is
the closest to me and click Review + create.
Now the Azure is running Validation, Validation has passed,
and click Create. Now you will be redirected to
the deployment progress page.
Now you can pause the video until the deployment is finished.
It has finished, now go to Resource and on the overview
page click on Containers and click Add new container.
And for container name input the name of the container
that will be Input. Click Create and we have created
a container input.
Now go to storage Explorer, click on Block containers.
Click on container
we just created the input, and create a new folder.
Okay, we will name it, uh, in the form of year, month, day,
and use your current day when you are watching this
task. Mine is this,
yours will be different, but that doesn't matter.
And click on OK.
Okay,
we have created the folder in the Container input.
Now go to Home and we will next create a destination Storage account.
So type in Storage.
Microsoft Storage Account, and click Create. Everything is
the same as before.
The only difference now is the name of the storage account.
Mine will be dstdsavovic, yours will be
different because it has to be globally unique, and click
review and create, validation is running, validation passed,
and click Create again.
You will be redirected to the deployment is in progress page.
You can now pause the video. It has finished;
now go to Resource and click on Containers, and create a new
container named Output. And we have created a container
named Output.
Now go to Home,
click Create a resource and type in Stream and click on Stream
Analytics job because we will now create Stream Analytics Job
that will transfer data from the source storage account
to destination storage account. Click Create.
And for job name I will
input SAJ as Stream Analytics Job and dsavovic.
So it's my first letter of first name and my last name.
For Resource group,
choose your resource group, mine is dsavovic;
for location
choose the closest location to you,
Mine is West Europe; for hosting environment ensure that it is
Cloud; for streaming unit decrease it to 1, like this.
Okay, and click Create. And deployment is in progress and it is
complete now. Now click on Go to resource and you
will be directed to the overview page of the newly created
Stream Analytics Job. Click on Inputs to create an input.
You don't have any input because this is a new Stream Analytics
Job. So let's create a new blob storage input like this.
And for input alias I will choose Enter input.
You can do the same.
I will provide blob storage settings manually.
You also, and my storage account
name is s a srcdsavovic.
Yours is different. Container
we created is Input.
This is the same for you; and authentication mode changed
to connection string.
Now we have to fetch storage account key of the 
sasrcdsavovic.
So go to top left, right-click
Home, and open link in your tab.
Go to that tab, which should open. Click on Storage accounts
and click on your source storage account.
Mine is sasrcdsavovic. Go to Access keys.
Click on Show keys and copy to clipboard
this Key 1.
Go back to the first tab and paste the contents of
the clipboard.
So just paste.
Okay. Path pattern leave blank.
Partition key also.
All the other settings leave to their defaults
and click on Save.
Okay, so we have added Input.
The input is now being tested.
Let's see if the test succeeded.
Successful connection test.
Okay, close this and go to Outputs.
Now we will create a Stream Analytics Job output, which is
also storage account, but destination storage account.
Click on Add and Blob storage/gen2.
Output
alias will be output.
You use the same.
You will provide storage settings manually.
Storage account, mine is sadestdsavovic,
yours is different.
And so for storage account key, we have to go to second tab
we opened
previously. Click on Storage Accounts
and choose your destination storage account.
Go to Access keys.
Click on Show keys,
and copy the primary key 1 to clipboard.
Go to the first tab where you left off and paste the storage
account key.
Okay, for container type in Output.
Path pattern leave blank and all other attributes leave
on their default settings.
Make sure that authentication mode is connection string
and click Save.
We have now added output and the Azure is testing
output to see if the container exists.
It should be done in a moment.
Yes. So the connection test is successful.
You can close this modification tab and go to query.
You have your query template, which doesn't...
It's not correct for our case because our output is named
Output and input is named Input.
So just change it.
So I'll type in Input here and here and you can save the query.
Okay.
Go to the overview of the Stream Analytics Job and click on Start.
Azure is asking us if we want to start it now.
Yes, we want, and click on Start.
So the streaming job is starting.
This will take some time to start.
We have a message here saying "Starting". You can now pause the
video and wait for the message "Running".
Okay, The job is now running, and we now can go to the second
tab. Click on Storage accounts, go to your source storage
account.
Go to Storage Explorer.
Go to Containers, Input, and...
We have to again create a folder with the format
year, month, day.
Okay.
You're now in that folder.
And now click Upload and point to desktop where I have put
some files for you in the Helper folder.
Choose input 01.json, and click Open and click Upload.
Okay, close this and close this.
We have uploaded
input 01.json.
We can see how that JSON looks like.
Open the Windows
Explorer.
Go to Desktop, Helper and double-click
input 01.json.
So this is the JSON we had just uploaded.
So the city's Reykjavik, we have  some latitude and longitude,
population, region and so on, you can close this.
Okay, go back to portal, to the browser.
Click on Storage Accounts.
Click on your destination storage account.
Go to Storage Explorer Preview.
Click Envelope containers, click on output container.
Here we have our JSON.
Double-click it and click OK to begin download.
It should download soon.
Let's click again.
Click OK to download.
I just want to show you the content of that file.
Let's go to overview
page.
Click on Containers, click on Output, click on the JSON file
and click Download here.
Okay, it has downloaded. Click on the file to open it, and you
see here is the same JSON we uploaded there.
It's on the second...
It's in the destination
storage account.
So in this task, you learned how to create an Azure Stream
Analytics Job,
how to configure an input,
write a transformation,
basic transformation query and configure output.
You used Azure Storage as your source and destination and
created a transformation query to produce some basic results.
In addition, you learned how to start a Stream Analytics Job and
view the job results. 
In the next task we'll start with building a Stream Analytics Job that
analyzes and visualizes data from a wind turbine.
Let's go to the first tab of the browser and to
the Stream Analytics Job and stop this job.
Click on Yes and see you in the next task.