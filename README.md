# Real time analytics using Azure

## Pre-requisites
1.  Please create an Azure Free Trial/student subscription beforehand at https://portal.azure.com
2. .NET Framework for Windows: https://dotnet.microsoft.com/en-us/download/dotnet-framework

## 1. Data streams and event processing

In the context of this project, data streams are event data generated by sensors or other sources that can be analyzed by another technology. Analyzing a data stream is typically done to measure the state change of a component or to capture information on an area of interest. The process of consuming data streams, analyzing them and deriving actionable insights out of them is called them Event Processing. It requires an event producer event processor and event consumer. 

### 1. Once you have access to your azure subscription, create a resource group
### 2. Create a source storage account and a destination storage account
### 3. Under storage resource, click on containers under the Data storage panel. 
### 4. Add a new container and give it an alias.
### 5. After creating container, click on storage browser in the left panel and select blob containers. Navigate to the container that you just created. We will now create a new folder in this container. Click on Add Directory and give it a name. We will name it in the current date format. **YYYY-MM-DD**
### 6. Repeat steps 2 to 4 to create a destination storage account and give it an alias.
### 7. Now, we will create a stream analytics job in Azure portal that will transfer data from source storage account to destination storage account. 
### 8. For hosting environment ensure it is set to Cloud and set streaming units to 1. Click create to create the job. 
### 9. Before proceeding to the next step, visit your source storage account. Select Access Keys under Security + Networking. Click ‘show’ for the key under ‘key1’ and copy this key. We will require this in the further steps.
### 10. Once the stream analytics job has been deployed, go to the resource. Select Inputs under Job topology. Since it is a new stream analytics job, you should see no inputs. Let’s add a new stream input. Click on add stream input dropdown and select Blob storage/ADLS Gen2. 
### 11. Under Blob storage/ADLS Gen2, enter your desired input alias, the source storage account name that you’d created. Change the authentication mode to connection string. For the storage account key, copy and paste the key that you had copied in step 9. Click Save.
### 12. Your input should look like this.
### 13. To create an output account, select ‘Outputs’ under Job Topology on the left panel. Follow instructions 9 to 11 to arrive at the desired result.
### 14. Your output should look like this.
### 15. Click on query under job topology. Verify your query by making sure that [output] and [input] matches name that you’ve assigned for your output storage and input storage respectively.
### 16. Go to the Overview tab and start the job.
### 17. Once the job has been successfully started, go to storage accounts > source storage account > containers > input. Here, you’ll see the folder that you’d created with the YYYY-MM-DD naming format. Select the folder and upload input-01.json. 
### 18. As the stream analytics job is running, you may go to storage accounts > destination storage account > containers > output. Here you’ll find a new json file that has been generated. On checking the file, we can verify that the contents in both the input and output json files are similar.  
